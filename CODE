import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.probability import FreqDist
from collections import defaultdict
from sklearn.naive_bayes import MultinomialNB
import string

# Download required NLTK data
nltk.download('punkt')
nltk.download('stopwords')

# Function to preprocess a tweet
def preprocess_tweet(tweet):
    tokens = word_tokenize(tweet.lower())
    tokens = [token.strip(string.punctuation) for token in tokens]
    tokens = [token for token in tokens if token.isalpha() and token not in stopwords.words('english')]
    return tokens


# Function to train the Naive Bayes model
def train_nb_model(tweets, labels):
    word_freq = defaultdict(FreqDist)
    for tokens, label in zip(tweets, labels):
        for token in tokens:
            word_freq[label][token] += 1
    prior_pos = sum(labels) / len(labels)
    prior_neg = 1 - prior_pos
    return word_freq, prior_pos, prior_neg

# Function to make predictions using the Naive Bayes model
def make_predictions(nb_model, tweets):
    word_freq, prior_pos, prior_neg = nb_model
    predictions = []
    for tokens in tweets:
        pos_prob = prior_pos
        neg_prob = prior_neg
        for token in tokens:
            pos_prob *= word_freq[1].freq(token)
            neg_prob *= word_freq[0].freq(token)
        predictions.append(1 if pos_prob > neg_prob else 0)
    return predictions

# Load the labeled tweets for training
labeled_tweets = [
    ("i nice to meet you.... thank you....", 1),
    ("This product is terrible.", 0),
    ("I'm so happy with this purchase.", 1),
    ("This company is awful.", 0),
    # Add more labeled tweets here
]
labeled_tokens = [preprocess_tweet(tweet) for tweet, _ in labeled_tweets]
labels = [label for _, label in labeled_tweets]

# Train the Naive Bayes model
nb_model = train_nb_model(labeled_tokens, labels)

# Load the input tweets from input.txt
with open('tweet.txt', 'r') as f:
    input_tweets = [line.strip() for line in f.readlines()]

# Preprocess the input tweets
input_tokens = [preprocess_tweet(tweet) for tweet in input_tweets]

# Make predictions
predictions = make_predictions(nb_model, input_tokens)

# Print the results
for i, tweet in enumerate(input_tweets):
    print(f'Tweet: {tweet}, Sentiment: {"Positive" if predictions[i] == 1 else "Negative"}')

# Compute ratios of positive words to negative words
positive_words = set()
negative_words = set()
for tokens, label in zip(labeled_tokens, labels):
    for token in tokens:
        if label == 1:
            positive_words.add(token)
        else:
            negative_words.add(token)
print(f'Ratio of positive words to negative words: {len(positive_words) / len(negative_words)}')

# Do some error analysis
error_count = 0
false_positives = 0
false_negatives = 0
true_positives = 0
true_negatives = 0
for i, (tweet, label) in enumerate(zip(labeled_tweets, labels)):
    prediction = make_predictions(nb_model, [preprocess_tweet(tweet[0])])[0]
    if prediction != label:
        error_count += 1
        if prediction == 1 and label == 0:
            false_positives += 1
        elif prediction == 0 and label == 1:
            false_negatives += 1
    else:
        if prediction == 1 and label == 1:
            true_positives += 1
        elif prediction == 0 and label == 0:
            true_negatives += 1
print(f'Error rate: {error_count / len(labeled_tweets)}')
print(f'False positive rate: {false_positives / len(labeled_tweets)}')
print(f'False negative rate: {false_negatives / len(labeled_tweets)}')
print(f'True positive rate: {true_positives / len(labeled_tweets)}')
print(f'True negative rate: {true_negatives / len(labeled_tweets)}')

# Analyze the most common positive and negative words
positive_word_freq = nb_model[0][1]
negative_word_freq = nb_model[0][0]
print("Most common positive words:")
for word, freq in positive_word_freq.most_common(10):
    print(f'{word}: {freq}')
print("Most common negative words:")
for word, freq in negative_word_freq.most_common(10):
    print(f'{word}: {freq}')

# Analyze the most informative features
positive_informative_features = [(word, freq) for word, freq in positive_word_freq.items() if word not in negative_word_freq]
negative_informative_features = [(word, freq) for word, freq in negative_word_freq.items() if word not in positive_word_freq]
print("Most informative positive features:")
for word, freq in sorted(positive_informative_features, key=lambda x: x[1], reverse=True)[:10]:
    print(f'{word}: {freq}')
print("Most informative negative features:")
for word, freq in sorted(negative_informative_features, key=lambda x: x[1], reverse=True)[:10]:
    print(f'{word}: {freq}')
